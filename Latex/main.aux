\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\providecommand \oddpage@label [2]{}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\citation{boerlin}
\citation{alemi2017learning}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\citation{alemi2017learning}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\@writefile{toc}{\contentsline {section}{\numberline {1}Learning in adaptive non-linear control theory}{5}{section.1}\protected@file@percent }
\newlabel{sec:learning_adap}{{1}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.1}{}}
\newlabel{eq:teacher_dyn}{{1}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.1.1}{}}
\newlabel{eq:student_dyn}{{2}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.1.2}{}}
\newlabel{eq:learning_rule}{{3}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning arbitrary dynamical systems in EBN's}{5}{section.2}\protected@file@percent }
\newlabel{sec:learning_rule}{{2}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.2}{}}
\citation{alemi2017learning}
\newlabel{eq:voltage_dyn}{{4}{6}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Training a classifier based on EBN's}{6}{section.3}\protected@file@percent }
\newlabel{eq:rate_dyn}{{5}{7}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A high-level illustration of the training (\textbf  {A}) and inference (\textbf  {B}) process using the presented framework for an auditory classification task. \textbf  {A:} During training, auditory samples are filtered using a set of band-pass filters with varying cut-off frequencies to produce a 16 channel input, denoted $\mathbf  {c}$. This input is then fed into the pre-trained rate network, which does inference on the presented signal. The dynamics $\mathbf  {x}$ produced by the rate network are then used to compute the error $\mathbf  {e} = \mathbf  {x} - \mathaccentV {hat}05E{\mathbf  {x}}$, which is then fed back into the spiking network using the feed-forward weights $\mathbf  {D}^T$. Simultaneously, the spiking network receives the same input as the rate network, simply projected by the feed-forward weights $\mathbf  {F} = \mathbf  {D}^T$. Using the read-out weights $\mathbf  {D}$, it then produces an estimation of the rate network dynamics $\mathaccentV {hat}05E{\mathbf  {x}}$, which is then used to compute the error. \newline  The inference mode, depicted in \textbf  {B}, is straightforward: The now trained spiking network receives the same input the rate network would receive, but transformed using the feed-forward weights $\mathbf  {F}$. It then produces the network dynamics $\mathaccentV {tilde}07E{\mathbf  {x}} = \mathbf  {D}r$ that closely match the ones of the rate network (if it would have received the signal) and produces the output $\mathaccentV {tilde}07E{\mathbf  {y}} = \mathaccentV {hat}05E{\mathbf  {D}}\mathaccentV {tilde}07E{\mathbf  {x}}$. The value $\mathaccentV {tilde}07E{\mathbf  {y}}$ is then thresholded to obtain the final prediction.}}{8}{figure.1}\protected@file@percent }
\newlabel{fig:figure1}{{1}{8}{A high-level illustration of the training (\textbf {A}) and inference (\textbf {B}) process using the presented framework for an auditory classification task. \textbf {A:} During training, auditory samples are filtered using a set of band-pass filters with varying cut-off frequencies to produce a 16 channel input, denoted $\mathbf {c}$. This input is then fed into the pre-trained rate network, which does inference on the presented signal. The dynamics $\mathbf {x}$ produced by the rate network are then used to compute the error $\mathbf {e} = \mathbf {x} - \hat {\mathbf {x}}$, which is then fed back into the spiking network using the feed-forward weights $\mathbf {D}^T$. Simultaneously, the spiking network receives the same input as the rate network, simply projected by the feed-forward weights $\mathbf {F} = \mathbf {D}^T$. Using the read-out weights $\mathbf {D}$, it then produces an estimation of the rate network dynamics $\hat {\mathbf {x}}$, which is then used to compute the error. \newline The inference mode, depicted in \textbf {B}, is straightforward: The now trained spiking network receives the same input the rate network would receive, but transformed using the feed-forward weights $\mathbf {F}$. It then produces the network dynamics $\tilde {\mathbf {x}} = \mathbf {D}r$ that closely match the ones of the rate network (if it would have received the signal) and produces the output $\tilde {\mathbf {y}} = \hat {\mathbf {D}}\tilde {\mathbf {x}}$. The value $\tilde {\mathbf {y}}$ is then thresholded to obtain the final prediction}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{9}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Temporal XOR}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {A:} The spiking network consisting of 300 neurons successfully reconstructs the dynamics of the rate network. \textbf  {B:} Due to the underlying balanced network, the membrane potentials show irregular and distributed spiking activity. [This is not the case yet, but should follow from the theory] \textbf  {C:} Our network predicts 98\% of 300 unseen samples correctly. Note that although the reconstruction of the dynamics is quite precise, the later obatined response matches the target response quite poorly. This is simply due to the fact that a 25-dimensional rate network was the smallest possible rate network we could train to solve the task, meaning that every dimension of the dynamics is necessary to compute the final output to a high precision. To see that this is actually the reason, consider the second experiment: The reconstruction of the 128-dimensional dynamics is much worse compared to the temporal XOR task, but the final output still matches the actual target quite well.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:figure2}{{2}{9}{\textbf {A:} The spiking network consisting of 300 neurons successfully reconstructs the dynamics of the rate network. \textbf {B:} Due to the underlying balanced network, the membrane potentials show irregular and distributed spiking activity. [This is not the case yet, but should follow from the theory] \textbf {C:} Our network predicts 98\% of 300 unseen samples correctly. Note that although the reconstruction of the dynamics is quite precise, the later obatined response matches the target response quite poorly. This is simply due to the fact that a 25-dimensional rate network was the smallest possible rate network we could train to solve the task, meaning that every dimension of the dynamics is necessary to compute the final output to a high precision. To see that this is actually the reason, consider the second experiment: The reconstruction of the 128-dimensional dynamics is much worse compared to the temporal XOR task, but the final output still matches the actual target quite well}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Wake-phrase detection}{10}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Robustness}{10}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {A:} Audio samples either containing a key phrase, like "Wake up!", or plain talking were presented to the network, which then had to decide whether the input was the target phrase or not. \textbf  {B:} To make classification easier, 16 butterworth filters with different cutoff frequencies were applied to create a 16-dimensional input signal. \textbf  {C:} For a sample containing the key phrase, our network output closely follows the output of the rate network and thus makes the correct decision. In this experiment, a threshold at 0.6 determined whether the prediction was positive or negative. \textbf  {D:} Negative samples on the other hand did not cause a rise in the output variable and were therefore correctly classified as negative samples. For this particular experiment, the rate network achieved 91\% on a held out test set containing 300 samples. The spiking network achieved 89.03\% test accuracy, while imitating the dynamics of the rate network (\textbf  {E}). \textbf  {F:} As expected, the spiking activity of the network is distributed and does not exhibit any output dependent patterns [This is not the case yet, but should follow from the theory]. For this experiment, 2000 neurons were used, but a network of 1000 neurons achieves similar performance.}}{11}{figure.3}\protected@file@percent }
\newlabel{fig:figure3}{{3}{11}{\textbf {A:} Audio samples either containing a key phrase, like "Wake up!", or plain talking were presented to the network, which then had to decide whether the input was the target phrase or not. \textbf {B:} To make classification easier, 16 butterworth filters with different cutoff frequencies were applied to create a 16-dimensional input signal. \textbf {C:} For a sample containing the key phrase, our network output closely follows the output of the rate network and thus makes the correct decision. In this experiment, a threshold at 0.6 determined whether the prediction was positive or negative. \textbf {D:} Negative samples on the other hand did not cause a rise in the output variable and were therefore correctly classified as negative samples. For this particular experiment, the rate network achieved 91\% on a held out test set containing 300 samples. The spiking network achieved 89.03\% test accuracy, while imitating the dynamics of the rate network (\textbf {E}). \textbf {F:} As expected, the spiking activity of the network is distributed and does not exhibit any output dependent patterns [This is not the case yet, but should follow from the theory]. For this experiment, 2000 neurons were used, but a network of 1000 neurons achieves similar performance}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {A:} Reconstruction of the 128-dimensional target dynamics (only first 10 dimensions are shown). \textbf  {B:} Here, one can observe the effect of the mismatch on the recurrent dynamics. Due to the distribution of the neuron- and synaptic time-constants, recurrent activity is sustained a little longer with some individual neurons spiking at a higher frequency. In the last network instance, we clamped 20\% of the neurons at a crucial time during classification. However, as can be seen in \textbf  {C}, the reconstruction is stable, as well as the final output. It should also be noted that for 20\% clamped neurons, the mean-squared error (MSE) did not increase during the time the neurons were shut off.}}{12}{figure.4}\protected@file@percent }
\newlabel{fig:figure4}{{4}{12}{\textbf {A:} Reconstruction of the 128-dimensional target dynamics (only first 10 dimensions are shown). \textbf {B:} Here, one can observe the effect of the mismatch on the recurrent dynamics. Due to the distribution of the neuron- and synaptic time-constants, recurrent activity is sustained a little longer with some individual neurons spiking at a higher frequency. In the last network instance, we clamped 20\% of the neurons at a crucial time during classification. However, as can be seen in \textbf {C}, the reconstruction is stable, as well as the final output. It should also be noted that for 20\% clamped neurons, the mean-squared error (MSE) did not increase during the time the neurons were shut off}{figure.4}{}}
\newlabel{table:table1}{{4.3}{13}{Robustness}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test accuracies over 5 trials on 1000 samples for networks perturbed by different kinds of noise. [NEEDS FIXING]}}{13}{table.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {A} shows an example of such a recording. \textbf  {B:} The membrane time-constants of neurons for different expected time-constants. It can be seen that as the time constant increases, also the standard deviation increases (thus $\sigma =0.2\mu $). The time-constants were computed by fitting an exponential to subthreshold recordings of each neuron.}}{13}{figure.5}\protected@file@percent }
\newlabel{fig:figure5}{{5}{13}{\textbf {A} shows an example of such a recording. \textbf {B:} The membrane time-constants of neurons for different expected time-constants. It can be seen that as the time constant increases, also the standard deviation increases (thus $\sigma =0.2\mu $). The time-constants were computed by fitting an exponential to subthreshold recordings of each neuron}{figure.5}{}}
\bibdata{bibliography}
\bibcite{alemi2017learning}{{1}{2017}{{Alemi et~al.}}{{Alemi, Machens, Denève, and Slotine}}}
\bibcite{boerlin}{{2}{2013}{{Boerlin et~al.}}{{Boerlin, Machens, and Denève}}}
\bibcite{Bourdoukan:2012:LOS:2999325.2999390}{{3}{2012}{{Bourdoukan et~al.}}{{Bourdoukan, Barrett, Machens, and Den\`{e}ve}}}
