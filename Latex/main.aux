\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\providecommand \oddpage@label [2]{}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\citation{boerlin}
\citation{alemi2017learning}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\citation{alemi2017learning}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\@writefile{toc}{\contentsline {section}{\numberline {1}Learning in adaptive non-linear control theory}{5}{section.1}\protected@file@percent }
\newlabel{sec:learning_adap}{{1}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.1}{}}
\newlabel{eq:teacher_dyn}{{1}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.1.1}{}}
\newlabel{eq:student_dyn}{{2}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.1.2}{}}
\newlabel{eq:learning_rule}{{3}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning arbitrary dynamical systems in EBN's}{5}{section.2}\protected@file@percent }
\newlabel{sec:learning_rule}{{2}{5}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.2}{}}
\citation{alemi2017learning}
\newlabel{eq:voltage_dyn}{{4}{6}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Training a classifier based on EBN's}{6}{section.3}\protected@file@percent }
\newlabel{eq:rate_dyn}{{5}{7}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A high-level illustration of the training (\textbf  {A}) and inference (\textbf  {B}) process using the presented framework for an auditory classification task. \textbf  {A:} During training, auditory samples are filtered using a set of band-pass filters with varying cut-off frequencies to produce a 16 channel input, denoted $\mathbf  {c}$. This input is then fed into the pre-trained rate network, which does inference on the presented signal. The dynamics $\mathbf  {x}$ produced by the rate network are then used to compute the error $\mathbf  {e} = \mathbf  {x} - \hat  {\mathbf  {x}}$, which is then fed back into the spiking network using the feed-forward weights $\mathbf  {D}^T$. Simultaneously, the spiking network receives the same input as the rate network, simply projected by the feed-forward weights $\mathbf  {F} = \mathbf  {D}^T$. Using the read-out weights $\mathbf  {D}$, it then produces an estimation of the rate network dynamics $\hat  {\mathbf  {x}}$, which is then used to compute the error. \newline  The inference mode, depicted in \textbf  {B}, is straightforward: The now trained spiking network receives the same input the rate network would receive, but transformed using the feed-forward weights $\mathbf  {F}$. It then produces the network dynamics $\tilde  {\mathbf  {x}} = \mathbf  {D}r$ that closely match the ones of the rate network (if it would have received the signal) and produces the output $\tilde  {\mathbf  {y}} = \hat  {\mathbf  {D}}\tilde  {\mathbf  {x}}$. The value $\tilde  {\mathbf  {y}}$ is then thresholded to obtain the final prediction.}}{8}{figure.1}\protected@file@percent }
\newlabel{fig:figure1}{{1}{8}{A high-level illustration of the training (\textbf {A}) and inference (\textbf {B}) process using the presented framework for an auditory classification task. \textbf {A:} During training, auditory samples are filtered using a set of band-pass filters with varying cut-off frequencies to produce a 16 channel input, denoted $\mathbf {c}$. This input is then fed into the pre-trained rate network, which does inference on the presented signal. The dynamics $\mathbf {x}$ produced by the rate network are then used to compute the error $\mathbf {e} = \mathbf {x} - \hat {\mathbf {x}}$, which is then fed back into the spiking network using the feed-forward weights $\mathbf {D}^T$. Simultaneously, the spiking network receives the same input as the rate network, simply projected by the feed-forward weights $\mathbf {F} = \mathbf {D}^T$. Using the read-out weights $\mathbf {D}$, it then produces an estimation of the rate network dynamics $\hat {\mathbf {x}}$, which is then used to compute the error. \newline The inference mode, depicted in \textbf {B}, is straightforward: The now trained spiking network receives the same input the rate network would receive, but transformed using the feed-forward weights $\mathbf {F}$. It then produces the network dynamics $\tilde {\mathbf {x}} = \mathbf {D}r$ that closely match the ones of the rate network (if it would have received the signal) and produces the output $\tilde {\mathbf {y}} = \hat {\mathbf {D}}\tilde {\mathbf {x}}$. The value $\tilde {\mathbf {y}}$ is then thresholded to obtain the final prediction}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{9}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Temporal XOR}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {A:} The spiking network consisting of 300 neurons successfully reconstructs the dynamics of the rate network. \textbf  {B:} Due to the underlying balanced network, the membrane potentials show irregular and distributed spiking activity. \textbf  {C:} Our network predicts 98\% of 300 unseen samples correctly. Note that although the reconstruction of the dynamics is quite precise, the later obatined response matches the target response quite poorly. This is simply due to the fact that a 25-dimensional rate network was the smallest possible rate network we could train to solve the task, meaning that every dimension of the dynamics is necessary to compute the final output to a high precision. To see that this is actually the reason, consider the second experiment: The reconstruction of the 128-dimensional dynamics is much worse compared to the temporal XOR task, but the final output still matches the actual target quite well.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:figure2}{{2}{9}{\textbf {A:} The spiking network consisting of 300 neurons successfully reconstructs the dynamics of the rate network. \textbf {B:} Due to the underlying balanced network, the membrane potentials show irregular and distributed spiking activity. \textbf {C:} Our network predicts 98\% of 300 unseen samples correctly. Note that although the reconstruction of the dynamics is quite precise, the later obatined response matches the target response quite poorly. This is simply due to the fact that a 25-dimensional rate network was the smallest possible rate network we could train to solve the task, meaning that every dimension of the dynamics is necessary to compute the final output to a high precision. To see that this is actually the reason, consider the second experiment: The reconstruction of the 128-dimensional dynamics is much worse compared to the temporal XOR task, but the final output still matches the actual target quite well}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Wake-phrase detection}{10}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces As $k$ is slowly decreasing, the helping error current $I_{kD^Te}$ becomes negligible and the network must generalize. During this process, the reconstruction error and the training accuracy increases/decreases a little bit. }}{10}{figure.3}\protected@file@percent }
\newlabel{fig:figure6}{{3}{10}{As $k$ is slowly decreasing, the helping error current $I_{kD^Te}$ becomes negligible and the network must generalize. During this process, the reconstruction error and the training accuracy increases/decreases a little bit}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {A:} Audio samples either containing a key phrase, like "Wake up!", or plain talking were presented to the network, which then had to decide whether the input was the target phrase or not. \textbf  {B:} To make classification easier, 16 butterworth filters with different cutoff frequencies were applied to create a 16-dimensional input signal. \textbf  {C:} For a sample containing the key phrase, our network output closely follows the output of the rate network and thus makes the correct decision. In this experiment, a threshold at 0.6 determined whether the prediction was positive or negative. \textbf  {D:} Negative samples on the other hand did not cause a rise in the output variable and were therefore correctly classified as negative samples. For this particular experiment, the rate network achieved 90.31\% on a held out test set containing 1000 samples. The spiking network achieved 86.80\% test accuracy, while imitating the dynamics of the rate network (\textbf  {E}). \textbf  {F:} As expected, the spiking activity of the network is distributed and does not exhibit any output dependent patterns. For this experiment, 1024 neurons were used.}}{11}{figure.4}\protected@file@percent }
\newlabel{fig:figure3}{{4}{11}{\textbf {A:} Audio samples either containing a key phrase, like "Wake up!", or plain talking were presented to the network, which then had to decide whether the input was the target phrase or not. \textbf {B:} To make classification easier, 16 butterworth filters with different cutoff frequencies were applied to create a 16-dimensional input signal. \textbf {C:} For a sample containing the key phrase, our network output closely follows the output of the rate network and thus makes the correct decision. In this experiment, a threshold at 0.6 determined whether the prediction was positive or negative. \textbf {D:} Negative samples on the other hand did not cause a rise in the output variable and were therefore correctly classified as negative samples. For this particular experiment, the rate network achieved 90.31\% on a held out test set containing 1000 samples. The spiking network achieved 86.80\% test accuracy, while imitating the dynamics of the rate network (\textbf {E}). \textbf {F:} As expected, the spiking activity of the network is distributed and does not exhibit any output dependent patterns. For this experiment, 1024 neurons were used}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Robustness}{12}{subsection.4.3}\protected@file@percent }
\newlabel{table:table1}{{4.3}{12}{Robustness}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Test accuracies over 13 trials on 1000 samples for a network of 768 neurons perturbed by different kinds of noise.}}{12}{figure.5}\protected@file@percent }
\newlabel{fig:figure7}{{5}{12}{Test accuracies over 13 trials on 1000 samples for a network of 768 neurons perturbed by different kinds of noise}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {A:} Reconstruction of the 128-dimensional target dynamics (only first 10 dimensions are shown). \textbf  {B:} Here, one can observe the effect of the mismatch on the recurrent dynamics. Due to the distribution of the neuron- and synaptic time-constants, recurrent activity is sustained a little longer with some individual neurons spiking at a higher frequency. In the last network instance, we clamped 20\% of the neurons at a crucial time during classification. However, as can be seen in \textbf  {C}, the reconstruction is stable, as well as the final output. It should also be noted that for 20\% clamped neurons, the mean-squared error (MSE) did not increase during the time the neurons were shut off.}}{13}{figure.6}\protected@file@percent }
\newlabel{fig:figure4}{{6}{13}{\textbf {A:} Reconstruction of the 128-dimensional target dynamics (only first 10 dimensions are shown). \textbf {B:} Here, one can observe the effect of the mismatch on the recurrent dynamics. Due to the distribution of the neuron- and synaptic time-constants, recurrent activity is sustained a little longer with some individual neurons spiking at a higher frequency. In the last network instance, we clamped 20\% of the neurons at a crucial time during classification. However, as can be seen in \textbf {C}, the reconstruction is stable, as well as the final output. It should also be noted that for 20\% clamped neurons, the mean-squared error (MSE) did not increase during the time the neurons were shut off}{figure.6}{}}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {A:} Recording of a neurons membrane potential on an analog mixed-signal neuromorphic chip. \textbf  {B:} The membrane time-constants of neurons for different expected time-constants. It can be seen that as the time constant increases, also the standard deviation increases (thus $\sigma =0.2\mu $). The time-constants were computed by fitting an exponential to subthreshold recordings of each neuron.}}{14}{figure.7}\protected@file@percent }
\newlabel{fig:figure5}{{7}{14}{\textbf {A:} Recording of a neurons membrane potential on an analog mixed-signal neuromorphic chip. \textbf {B:} The membrane time-constants of neurons for different expected time-constants. It can be seen that as the time constant increases, also the standard deviation increases (thus $\sigma =0.2\mu $). The time-constants were computed by fitting an exponential to subthreshold recordings of each neuron}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Mismatch robustness of a network with 1024 (l) and 768 (r) neurons.}}{14}{figure.8}\protected@file@percent }
\newlabel{fig:figure8}{{8}{14}{Mismatch robustness of a network with 1024 (l) and 768 (r) neurons}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Role of $\mathbf  {\Omega ^f}$}{14}{subsection.4.4}\protected@file@percent }
\citation{alemi2017learning}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training accuracy (top) and reconstruction error (bottom) over the course of training for a network with and without $\mathbf  {\Omega ^f}$.}}{15}{figure.9}\protected@file@percent }
\newlabel{fig:figure9}{{9}{15}{Training accuracy (top) and reconstruction error (bottom) over the course of training for a network with and without $\mathbf {\Omega ^f}$}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The reconstruction error of the "clamped" network with $\mathbf  {\Omega ^f}$ is constantly lower than the one without $\mathbf  {\Omega ^f}$, while the reconstruction error for the original network without $\mathbf  {\Omega ^f}$ has lower reconstruction error and therefore better classification accuracy.}}{15}{figure.10}\protected@file@percent }
\newlabel{fig:figure10}{{10}{15}{The reconstruction error of the "clamped" network with $\mathbf {\Omega ^f}$ is constantly lower than the one without $\mathbf {\Omega ^f}$, while the reconstruction error for the original network without $\mathbf {\Omega ^f}$ has lower reconstruction error and therefore better classification accuracy}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{16}{section.5}\protected@file@percent }
\bibdata{bibliography}
\bibcite{alemi2017learning}{{1}{2017}{{Alemi et~al.}}{{Alemi, Machens, Denève, and Slotine}}}
\bibcite{boerlin}{{2}{2013}{{Boerlin et~al.}}{{Boerlin, Machens, and Denève}}}
\bibcite{Bourdoukan:2012:LOS:2999325.2999390}{{3}{2012}{{Bourdoukan et~al.}}{{Bourdoukan, Barrett, Machens, and Den\`{e}ve}}}
